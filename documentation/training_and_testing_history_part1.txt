Windows PowerShell
Copyright (C) Microsoft Corporation. Alle Rechte vorbehalten.

Lernen Sie das neue plattformübergreifende PowerShell kennen – https://aka.ms/pscore6      

PS C:\Users\sidla\Documents\GitHub\machine-translation> & c:/Users/sidla/Documents/GitHub/machine-translation/.venv/Scripts/Activate.ps1
(.venv) PS C:\Users\sidla\Documents\GitHub\machine-translation> & c:/Users/sidla/Documents/GitHub/machine-translation/.venv/Scripts/python.exe
Python 3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)] on 
win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import string # for string manipulation during pre-processing
>>> import string # for string manipulation during pre-processing
>>> import pandas as pd # for data frames
>>> import numpy as np # for calculations
>>> import matplotlib.pyplot as plt # for plotting
>>> from keras.models import Sequential # for building encoder decoder model with RNNs
2022-08-07 01:57:01.939657: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found    
2022-08-07 01:57:01.940308: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
>>> from keras.layers import RepeatVector, GRU, Dense, TimeDistributed, Embedding # for building encoder decoder model with RNNs
>>> from keras.preprocessing.text import Tokenizer # for pre-processing
>>> from keras_preprocessing.sequence import pad_sequences # for pre-processing
>>> from keras.initializers import Constant # for matrix with pre-trained embedding weights
>>> from keras_self_attention import SeqSelfAttention as Attention # for attention mechanism
>>> from gensim.models import KeyedVectors # to load in pretrained word2vec weights
>>> import seaborn as sns # for creating boxplots
>>> from collections import Counter # for further array functionality
>>> import dataframe_image as dfi # to store data frames as images
>>> # import corpus
>>>
>>> eng = pd.read_csv("data/europarl-v7.nl-en.en", sep="\t", names=["eng"])
>>> nl = pd.read_csv("data/europarl-v7.nl-en.nl", sep="\t", names=["nl"], on_bad_lines='skip') #bad lines are skiped due to pandas.errors.ParserError: Expected 1 fields in line 952982, saw 2
>>> corpus = pd.DataFrame(eng)
>>> corpus["nl"] = nl
>>> # TASK 1: analysis of corpus
>>> # array of all words in corpus
>>>
>>> words_eng = [word for sentence in corpus.eng for word in sentence.split()]
>>> words_nl = [word for sentence in corpus.nl for word in sentence.split()]
>>> # array of sentences' length
>>>
>>> num_word_per_sentence_eng = [len(sentence.split()) for sentence in corpus.eng]
>>> num_word_per_sentence_nl = [len(sentence.split()) for sentence in corpus.nl]
>>> # visualise sentence length
>>>
>>> data = pd.concat([
...     pd.DataFrame({"Number of words":num_word_per_sentence_nl, "Language":"Dutch"}),    
...     pd.DataFrame({"Number of words":num_word_per_sentence_eng, "Language":"English"})  
... ])
>>> 
>>> boxplot = sns.boxplot(y = "Number of words", x="Language", data=data).set_title('Length of English and Dutch sentences')
>>> fig = boxplot.figure.savefig("documentation/plots/summary_corpus/boxplot_number_words.png", format="png")
>>> plt.clf()
>>> # visualise top 10 most common used words
>>>
>>> plt.bar([item[0] for item in Counter(words_eng).most_common(10)], [item[1] for item in 
Counter(words_eng).most_common(10)])
<BarContainer object of 10 artists>
>>> plt.title('Top 10 most used English words')  
Text(0.5, 1.0, 'Top 10 most used English words')
>>> plt.savefig("documentation/plots/summary_corpus/top10_eng_words.png", format="png")
>>> plt.clf()
>>> plt.bar([item[0] for item in Counter(words_nl).most_common(10)], [item[1] for item in Counter(words_nl).most_common(10)])
<BarContainer object of 10 artists>
>>> plt.title('Top 10 most used English words')  
Text(0.5, 1.0, 'Top 10 most used English words')
>>> plt.savefig("documentation/plots/summary_corpus/top10_nl_words.png", format="png")
>>> plt.clf()
>>> # store values for documentation
>>>
>>> summary_corpus = pd.DataFrame({
...     "Language":["English", "Dutch"], 
...     "Number of sentences" : [len(corpus.eng), len(corpus.nl)],
...     "Number of words" : [len(words_eng), len(words_nl)],
...     "Minimum number of words per sentence" : [min(num_word_per_sentence_eng), min(num_word_per_sentence_nl)],
...     "Average number of words per sentence" : [np.mean(num_word_per_sentence_eng), np.mean(num_word_per_sentence_nl)],
...     "Maximum number of words per sentence" : [max(num_word_per_sentence_eng), max(num_word_per_sentence_nl)],
... }) 
>>> 
>>> dfi.export(summary_corpus, "documentation/tables_as_image/summary_corpus.png")
>>> # select random data of corpus for tasks 2-4
>>>
>>> def get_suitable_sample(): 
...     # due to limited resources select random 1% of corpus
...     sample = corpus.sample(round(len(corpus) * 0.01)) 
...     # due to limited CPU no outlier sentences in terms of number of words can be present in sample (otherwise too big input matrices for models)
...     eng_max_num_word_per_sentence = max([len(sentence.split()) for sentence in sample.eng])
...     nl_max_num_word_per_sentence = max([len(sentence.split()) for sentence in sample.nl])
...     if eng_max_num_word_per_sentence > 500 or nl_max_num_word_per_sentence > 500:
...         sample = get_suitable_sample() # if outliers are present re-call function till 
suitable sample found
...     return sample
... # due to limited resources use conditional random sampling
...
>>> sample = get_suitable_sample() 
>>> # transform english and dutch corpus into single numpy arrays for further steps
>>>
>>> eng_sentences = np.array(sample.eng)
>>> nl_sentences = np.array(sample.nl)
>>> # TASK 2: pre-pprocessing
>>>
>>> def clean_sentence(sentence):
...     # lower case sentences
...     lower_case_sent = sentence.lower()
...     # remove punctuation
...     clean_sentence = lower_case_sent.translate(str.maketrans('', '', string.punctuation))
...     return clean_sentence
...
>>> def tokenize(sentences, character_based = False):
...     # Create tokenizer
...     text_tokenizer = Tokenizer(char_level=character_based)
...     # Fit texts
...     text_tokenizer.fit_on_texts(sentences)
...     return text_tokenizer.texts_to_sequences(sentences), text_tokenizer
... # clean sentences
...
>>> eng_sentences_clean = [clean_sentence(sentence) for sentence in eng_sentences]
>>> nl_sentences_clean = [clean_sentence(sentence) for sentence in nl_sentences]
>>> # word-based tokenization
>>>
>>> nl_text_tokenized_word, nl_text_tokenizer_word = tokenize(nl_sentences_clean)
>>> eng_text_tokenized_word, eng_text_tokenizer_word = tokenize(eng_sentences_clean)
>>> # character-based tokenization
>>>
>>> nl_text_tokenized_char, nl_text_tokenizer_char = tokenize(nl_sentences_clean, character_based=True)
>>> eng_text_tokenized_char, eng_text_tokenizer_char = tokenize(eng_sentences_clean, character_based=True)
>>> # respective max sentence length in words or characters: crucial for padding and building all upcoming models
>>>
>>> max_nl_len_word = int(len(max(nl_text_tokenized_word,key=len)))
>>> max_eng_len_word = int(len(max(eng_text_tokenized_word,key=len)))
>>> max_nl_len_char = int(len(max(nl_text_tokenized_char,key=len)))
>>> max_eng_len_char = int(len(max(eng_text_tokenized_char,key=len)))
>>> # padding to respective max sentence length
>>>
>>> nl_pad_sentence_word = pad_sequences(nl_text_tokenized_word, max_nl_len_word, padding = "post")
>>> eng_pad_sentence_word = pad_sequences(eng_text_tokenized_word, max_eng_len_word, padding = "post")
>>> nl_pad_sentence_char = pad_sequences(nl_text_tokenized_char, max_nl_len_char, padding = "post")
>>> eng_pad_sentence_char = pad_sequences(eng_text_tokenized_char, max_eng_len_char, padding = "post")
>>> # reshape to 3D due to Keras' requirements
>>>
>>> nl_pad_sentence_word = nl_pad_sentence_word.reshape(*nl_pad_sentence_word.shape, 1)
>>> eng_pad_sentence_word = eng_pad_sentence_word.reshape(*eng_pad_sentence_word.shape, 1)
>>> nl_pad_sentence_char = nl_pad_sentence_char.reshape(*nl_pad_sentence_char.shape, 1)
>>> eng_pad_sentence_char = eng_pad_sentence_char.reshape(*eng_pad_sentence_char.shape, 1)
>>> # amount of unique words: crucial for building all upcoming models
>>>
>>> nl_vocab = len(nl_text_tokenizer_word.word_index) + 1
>>> eng_vocab = len(eng_text_tokenizer_word.word_index) + 1
>>> nl_char = len(nl_text_tokenizer_char.word_index) + 1
>>> eng_char = len(eng_text_tokenizer_char.word_index) + 1
>>> # summary of sample for documentation purposes
>>>
>>> eng_avg_len = np.mean([len(sentence) for sentence in eng_text_tokenized_word]) # average amount of words in a sentence of englisch corpus
>>> eng_num_word = sum([len(sentence) for sentence in eng_text_tokenized_word]) # number of words in english corpus
>>> eng_max_len = len(max(eng_text_tokenized_word,key=len)) # maximum sentence lenght in english corpus
>>> eng_min_len = len(min(eng_text_tokenized_word,key=len)) # minimum sentence lenght in english corpus
>>> nl_avg_len = np.mean([len(sentence) for sentence in nl_text_tokenized_word]) # average 
amount of words in a sentence of dutch corpus
>>> nl_num_word = sum([len(sentence) for sentence in nl_text_tokenized_word]) # number of words in dutch corpus
>>> nl_max_len = len(max(nl_text_tokenized_word,key=len)) # maximum sentence lenght in dutch corpus
>>> nl_min_len = len(min(nl_text_tokenized_word,key=len)) # minimum sentence lenght in dutch corpus
>>> summary_sample = pd.DataFrame({
...     "Language":["English", "Dutch"], 
...     "Number of sentences" : [len(sample.eng), len(sample.nl)],
...     "Minimum number of words per sentence" : [eng_min_len, nl_min_len],
...     "Average number of words per sentence" : [eng_avg_len, nl_avg_len],
...     "Maximum number of words per sentence" : [eng_max_len, nl_max_len],
...     "Number of words" : [eng_num_word, nl_num_word],
...     "Number of words (no duplicates)" : [eng_vocab, nl_vocab], 
...     "Nmber of used characters (no duplicates)" : [eng_char, nl_char],
... })
>>>
>>> dfi.export(summary_sample, "documentation/tables_as_image/summary_sample.png")
>>> # TASK 3: 
>>> # comparison of performance of neural machine translator depending word embedding (no embedding, Keras' embedding, GloVe, Word2Vec) and tokenization level (word or character)    
>>> # no embedding:
>>> # function for building ecoder-decoder model without embedding (only one-hot representation)
>>>
>>> def encdec_model(input_shape, output_max_len, output_vocab_size):
...     model = Sequential()
...     model.add(GRU(64, input_shape = input_shape[1:], return_sequences = False))
...     model.add(RepeatVector(output_max_len))
...     model.add(GRU(64, return_sequences = True))
...     model.add(TimeDistributed(Dense(output_vocab_size, activation = 'softmax')))
...     model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
...     return model
... # build word-based english to dutch and dutch to english translator
...
>>> eng2nl_encdec_word_model = encdec_model(eng_pad_sentence_word.shape, max_nl_len_word, nl_vocab)
2022-08-07 01:58:10.475234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found
2022-08-07 01:58:10.475683: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-08-07 01:58:10.481991: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LAPTOP-4KF1PS0S
2022-08-07 01:58:10.482418: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LAPTOP-4KF1PS0S
2022-08-07 01:58:10.484325: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.>>> nl2eng_encdec_word_model = encdec_model(nl_pad_sentence_word.shape, max_eng_len_word, eng_vocab)
>>> # build character-based english to dutch and dutch to english translator
>>>
>>> eng2nl_encdec_char_model = encdec_model(eng_pad_sentence_char.shape, max_nl_len_char, nl_char)
>>> nl2eng_encdec_char_model = encdec_model(nl_pad_sentence_char.shape, max_eng_len_char, eng_char)
>>> # own embedding model trough Keras:
>>> # function for building ecoder-decoder model with Keras' embedding model
>>>
>>> def keras_embd_encdec_model(input_shape, output_max_len, output_vocab_size, input_vocab_size):
...     model = Sequential()
...     model.add(Embedding(input_vocab_size, 300, input_length=input_shape[1], trainable = True)) #output dimension is set to 300 in order to be comparable with fixed output dim of 
glove and word2vec embedding
...     model.add(GRU(64, return_sequences = False))
...     model.add(RepeatVector(output_max_len))
...     model.add(GRU(64, return_sequences = True))
...     model.add(TimeDistributed(Dense(output_vocab_size, activation = 'softmax')))
...     model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
...     return model
... # build word-based english to dutch and dutch to english translator with keras embedding
...
>>> eng2nl_keras_embd_encdec_word_model = keras_embd_encdec_model(eng_pad_sentence_word.shape, max_nl_len_word, nl_vocab, eng_vocab)
>>> nl2eng_keras_embd_encdec_word_model = keras_embd_encdec_model(nl_pad_sentence_word.shape, max_eng_len_word, eng_vocab, nl_vocab)
>>> # build character-based english to dutch and dutch to english translator with keras embedding
>>>
>>> eng2nl_keras_embd_encdec_char_model = keras_embd_encdec_model(eng_pad_sentence_char.shape, max_nl_len_char, nl_char, eng_char)
>>> nl2eng_keras_embd_encdec_char_model = keras_embd_encdec_model(nl_pad_sentence_char.shape, max_eng_len_char, eng_char, nl_char)
>>> # glove embedding:
>>> # get all of glove's pre-trained weights for english words (source: https://nlp.stanford.edu/projects/glove/)
>>>
>>> eng_glove_word_vectors = {}
>>> with open("data\glove.6B.300d.txt", encoding="utf8") as glove_data:
...     for element in glove_data:
...         values = element.split()
...         word = values[0]
...         coefs = values[1:]
...         eng_glove_word_vectors[word] = np.asarray(coefs, dtype='float32')
... # select glove vectors whose words are also present in the english corpus and store it 
in a matrix
...
>>> eng_glove_embedding_matrix = np.zeros((eng_vocab, 300)) # 300 = word vector dimension = number of weights per word in glove
>>> eng_words_not_in_glove = []
>>> for word, i in eng_text_tokenizer_word.word_index.items():        
...     embedding_vector = eng_glove_word_vectors.get(word)
...     if embedding_vector is not None: # non exisiting words will be zero
...         eng_glove_embedding_matrix[i] = embedding_vector
...     else : 
...         eng_words_not_in_glove.append(word)
... # word2vec embedding:
... # get all of word2vec's pre-trained weights for english words (source: https://code.google.com/archive/p/word2vec/)
...
>>> eng_word2vec = KeyedVectors.load_word2vec_format("data\GoogleNews-vectors-negative300.bin", binary=True)
>>> # select word2vec vectors whose words are also present in the english corpus and store 
it in a matrix
>>>
>>> eng_word2vec_embedding_matrix = np.zeros((eng_vocab, 300)) # 300 = word2vec vector dimension (= 300 weigths per word)
>>> eng_words_not_in_word2vec = []
>>> for word, i in eng_text_tokenizer_word.word_index.items():  
...     if word in eng_word2vec : 
...         eng_word2vec_embedding_matrix[i] = eng_word2vec[word]
...     else : 
...         eng_words_not_in_word2vec.append(word)
... # get all of word2vec's pre-trained weights for dutch words (source: https://github.com/clips/dutchembeddings):
...
>>> nl_word2vec = KeyedVectors.load_word2vec_format("data\wikipedia-320.txt") # get all pretrained weigths
>>> # select word2vec vectors whose words are also present in the dutch corpus and store it in a matrix
>>> 
>>> nl_word2vec_embedding_matrix = np.zeros((nl_vocab, 320)) # 320 = word2vec vector dimension (= 320 weigths per word)
>>> nl_words_not_in_word2vec = []
>>> for word, i in nl_text_tokenizer_word.word_index.items():  
...     if word in nl_word2vec : 
...         nl_word2vec_embedding_matrix[i] = nl_word2vec[word]
...     else : 
...         nl_words_not_in_word2vec.append(word)
... # function for building encoder-decoder model with pretrained embedding weights
...
>>> def embd_encdec_model(input_shape, output_max_len, output_vocab_size, input_vocab_size, embedding_matrix):
...     #build model
...     model = Sequential()
...     model.add(Embedding(input_vocab_size, embedding_matrix.shape[1], input_length=input_shape[1], embeddings_initializer=Constant(embedding_matrix)))
...     model.add(GRU(64, return_sequences = False))
...     model.add(RepeatVector(output_max_len))
...     model.add(GRU(64, return_sequences = True))
...     model.add(TimeDistributed(Dense(output_vocab_size, activation = 'softmax')))
...     model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
...     return model
... # build eng2nl and nl2eng machine translator based on words and using pre-trained glove and word2vec weights
...
>>> eng2nl_glove_embd_encdec_word_model = embd_encdec_model(eng_pad_sentence_word.shape, max_nl_len_word, nl_vocab, eng_vocab, eng_glove_embedding_matrix)
>>> eng2nl_word2vec_embd_encdec_word_model = embd_encdec_model(eng_pad_sentence_word.shape, max_nl_len_word, nl_vocab, eng_vocab, eng_word2vec_embedding_matrix)
>>> nl2eng_word2vec_embd_encdec_word_model = embd_encdec_model(nl_pad_sentence_word.shape, 
max_eng_len_word, eng_vocab, nl_vocab, nl_word2vec_embedding_matrix)
>>> # TASK 4: neural machine translation with attention
>>> # function for building encoder-decoder model with pretrained embedding weights and attention mechanism (source:https://pypi.org/project/keras-self-attention/0.0.10/)
>>>
>>> def attention_embd_encdec_model(input_shape, output_max_len, output_vocab_size, input_vocab_size, embedding_matrix):
...     model = Sequential()
...     model.add(Embedding(input_vocab_size, embedding_matrix.shape[1], input_length=input_shape[1], embeddings_initializer=Constant(embedding_matrix)))
...     model.add(GRU(64, return_sequences = False))
...     model.add(RepeatVector(output_max_len))
...     model.add(Attention())
...     model.add(GRU(64, return_sequences = True))
...     model.add(TimeDistributed(Dense(output_vocab_size, activation = 'softmax')))
...     model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
...     return model
... # build models based on words with attention mechanism and pre-trained glove and word2vec weights
...
>>> eng2nl_attention_glove_embd_encdec_word_model = attention_embd_encdec_model(eng_pad_sentence_word.shape, max_nl_len_word, nl_vocab, eng_vocab, eng_glove_embedding_matrix)        
>>> eng2nl_attention_word2vec_embd_encdec_word_model = attention_embd_encdec_model(eng_pad_sentence_word.shape, max_nl_len_word, nl_vocab, eng_vocab, eng_word2vec_embedding_matrix)  
>>> nl2eng_attention_word2vec_embd_encdec_word_model = attention_embd_encdec_model(nl_pad_sentence_word.shape, max_eng_len_word, eng_vocab, nl_vocab, nl_word2vec_embedding_matrix)   
>>> # the essential models for task 3
>>> # - eng2nl glove --> 
>>> # - eng2nl word2vec -->
>>> # - nl2eng word2vec --> weights: https://github.com/clips/dutchembeddings
>>> # - eng2nl char based
>>> #essential models for task 4
>>> # - eng2nl word2vec with attention
>>>
>>> model_training_manuals = [
...     {
...         "title" : "Dutch to English translator (word-based, Word2Vec embedding)",
...         "model" : nl2eng_word2vec_embd_encdec_word_model,
...         "X" : nl_pad_sentence_word,
...         "y" : eng_pad_sentence_word
...     },
...        {   
...         "title" : "Dutch to English translator (word-based, Word2Vec embedding, with attention",
...         "model" : nl2eng_attention_word2vec_embd_encdec_word_model,
...         "X" : nl_pad_sentence_word,
...         "y" : eng_pad_sentence_word
...     },
...     {   
...         "title" : "English to Dutch translator (word-based, Word2Vec embedding, with attention)",
...         "model" : eng2nl_attention_word2vec_embd_encdec_word_model,
...         "X" : eng_pad_sentence_word,
...         "y" : nl_pad_sentence_word
...     },
...     {   
...         "title" : "English to Dutch translator (word-based, Glove embedding, with attention)",
...         "model" : eng2nl_attention_glove_embd_encdec_word_model,
...         "X" : eng_pad_sentence_word,
...         "y" : nl_pad_sentence_word
...     },
...     {   
...         "title" : "English to Dutch translator (word-based)",
...         "model" : eng2nl_encdec_word_model,
...         "X" : eng_pad_sentence_word,
...         "y" : nl_pad_sentence_word
...     },
...     {   
...         "title" : "English to Dutch translator (word-based, Word2Vec embedding)",      
...         "model" : eng2nl_word2vec_embd_encdec_word_model,
...         "X" : eng_pad_sentence_word,
...         "y" : nl_pad_sentence_word
...     },
...     {
...         "title" : "English to Dutch translator (char-based)",
...         "model" : eng2nl_encdec_char_model,
...         "X" : eng_pad_sentence_char,
...         "y" : nl_pad_sentence_char
...     },
...     {
...         "title" : "Dutch to English translator (word-based)",
...         "model" : nl2eng_encdec_word_model,
...         "X" : nl_pad_sentence_word,
...         "y" : eng_pad_sentence_word
...     },
...     {
...         "title" :"Dutch to English translator (char-based)",
...         "model" : nl2eng_encdec_char_model,
...         "X" : nl_pad_sentence_char,
...         "y" : eng_pad_sentence_char
...     }, 
...     {
...         "title" : "Dutch to English translator (word-based, Keras' embedding)",        
...         "model" : nl2eng_keras_embd_encdec_word_model,
...         "X" : nl_pad_sentence_word,
...         "y" : eng_pad_sentence_word
...     },
...     {   
...         "title" : "English to Dutch translator (word-based, Keras' embedding)",        
...         "model" : eng2nl_keras_embd_encdec_word_model,
...         "X" : eng_pad_sentence_word,
...         "y" : nl_pad_sentence_word
...     },
...     {
...         "title" : "English to Dutch translator (char-based, Keras' embedding)",        
...         "model" : eng2nl_keras_embd_encdec_char_model,
...         "X" : eng_pad_sentence_char,
...         "y" : nl_pad_sentence_char
...     },
...     {
...         "title" :"Dutch to English translator (char-based, Keras' embedding)",
...         "model" : nl2eng_keras_embd_encdec_char_model,
...         "X" : nl_pad_sentence_char,
...         "y" : eng_pad_sentence_char
...     },
...        {   
...         "title" : "English to Dutch translator (word-based, Glove embedding)",
...         "model" : eng2nl_glove_embd_encdec_word_model,
...         "X" : eng_pad_sentence_word,
...         "y" : nl_pad_sentence_word
...     },
... ]
>>>
>>> for manual in model_training_manuals:
...     print("\t\t\t")
...     print("MODEL: "+manual["title"])
...     # show model architecture
...     manual["model"].summary()
...     # train and test model
...     history = manual["model"].fit(manual["X"], manual["y"], batch_size=16, epochs=3, validation_split=0.2)
...     # accuracy of validation of last epoch
...     print("Last measured accuracy score: ", history.history["val_accuracy"][2])        
...     # visualise accuracy history
...     plt.plot(history.history['accuracy'])
...     plt.plot(history.history['val_accuracy'])
...     plt.title('Accuracy of '+manual["title"])
...     plt.ylabel('accuracy')
...     plt.xlabel('epoch')
...     plt.legend(['train', 'test'], loc='upper left')
...     plt.savefig("documentation/plots/accuracy/"+manual["title"]+".png", format="png")  
...     plt.clf()
...     # summarize history for loss
...     plt.plot(history.history['loss'])
...     plt.plot(history.history['val_loss'])
...     plt.title('model loss')
...     plt.ylabel('loss')
...     plt.xlabel('epoch')
...     plt.legend(['train', 'test'], loc='upper left')
...     plt.savefig("documentation/plots/loss/"+manual["title"]+".png", format="png")      
...     plt.clf()
... #function for converting tokenized data to text
...

MODEL: Dutch to English translator (word-based, Word2Vec embedding)
Model: "sequential_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 embedding_6 (Embedding)     (None, 212, 320)          8208960

 gru_20 (GRU)                (None, 64)                74112

 repeat_vector_10 (RepeatVec  (None, 211, 64)          0
 tor)

 gru_21 (GRU)                (None, 211, 64)           24960

 time_distributed_10 (TimeDi  (None, 211, 18108)       1177020
 stributed)

=================================================================
Total params: 9,485,052
Trainable params: 9,485,052
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
2022-08-07 02:03:06.524402: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 244530432 exceeds 10% of free system memory.
2022-08-07 02:03:06.767837: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 244530432 exceeds 10% of free system memory.
2022-08-07 02:03:06.873153: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 244530432 exceeds 10% of free system memory.
2022-08-07 02:03:07.011707: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 244530432 exceeds 10% of free system memory.
2022-08-07 02:03:07.824701: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 244530432 exceeds 10% of free system memory.
964/964 [==============================] - 1831s 2s/step - loss: 1.3183 - accuracy: 0.8824 
- val_loss: 0.9181 - val_accuracy: 0.8816
Epoch 2/3
964/964 [==============================] - 1795s 2s/step - loss: 0.9032 - accuracy: 0.8829 
- val_loss: 0.9087 - val_accuracy: 0.8829
Epoch 3/3
964/964 [==============================] - 1794s 2s/step - loss: 0.8962 - accuracy: 0.8840 
- val_loss: 0.9059 - val_accuracy: 0.8833
Last measured accuracy score:  0.8832729458808899
[<matplotlib.lines.Line2D object at 0x000002E9B5F96590>]
[<matplotlib.lines.Line2D object at 0x000002E9B5F96740>]
Text(0.5, 1.0, 'Accuracy of Dutch to English translator (word-based, Word2Vec embedding)') 
Text(0, 0.5, 'accuracy')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9B5F66560>
[<matplotlib.lines.Line2D object at 0x000002E9B5FE68C0>]
[<matplotlib.lines.Line2D object at 0x000002E9B5FE6BC0>]
Text(0.5, 1.0, 'model loss')
Text(0, 0.5, 'loss')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E7F38516C0>

MODEL: Dutch to English translator (word-based, Word2Vec embedding, with attention
Model: "sequential_13"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 embedding_9 (Embedding)     (None, 212, 320)          8208960

 gru_26 (GRU)                (None, 64)                74112

 repeat_vector_13 (RepeatVec  (None, 211, 64)          0
 tor)

 seq_self_attention_2 (SeqSe  (None, 211, 64)          4161
 lfAttention)

 gru_27 (GRU)                (None, 211, 64)           24960

 time_distributed_13 (TimeDi  (None, 211, 18108)       1177020
 stributed)

=================================================================
Total params: 9,489,213
Trainable params: 9,489,213
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
964/964 [==============================] - 1975s 2s/step - loss: 1.3192 - accuracy: 0.8824 
- val_loss: 0.9197 - val_accuracy: 0.8816
Epoch 2/3
964/964 [==============================] - 1941s 2s/step - loss: 0.9050 - accuracy: 0.8826 
- val_loss: 0.9115 - val_accuracy: 0.8824
Epoch 3/3
964/964 [==============================] - 1940s 2s/step - loss: 0.8983 - accuracy: 0.8838 
- val_loss: 0.9085 - val_accuracy: 0.8832
Last measured accuracy score:  0.8831697106361389
[<matplotlib.lines.Line2D object at 0x000002E9BA8A84F0>]
[<matplotlib.lines.Line2D object at 0x000002E9BA8A87F0>]
Text(0.5, 1.0, 'Accuracy of Dutch to English translator (word-based, Word2Vec embedding, with attention')
Text(0, 0.5, 'accuracy')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BA895F90>
[<matplotlib.lines.Line2D object at 0x000002E9BA8E8070>]
[<matplotlib.lines.Line2D object at 0x000002E9BA8E8370>]
Text(0.5, 1.0, 'model loss')
Text(0, 0.5, 'loss')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BA8C7E80>

MODEL: English to Dutch translator (word-based, Word2Vec embedding, with attention)
Model: "sequential_12"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 embedding_8 (Embedding)     (None, 211, 300)          5432400

 gru_24 (GRU)                (None, 64)                70272

 repeat_vector_12 (RepeatVec  (None, 212, 64)          0
 tor)

 seq_self_attention_1 (SeqSe  (None, 212, 64)          4161
 lfAttention)

 gru_25 (GRU)                (None, 212, 64)           24960

 time_distributed_12 (TimeDi  (None, 212, 25653)       1667445
 stributed)

=================================================================
Total params: 7,199,238
Trainable params: 7,199,238
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
964/964 [==============================] - 2572s 3s/step - loss: 1.3757 - accuracy: 0.8815 
- val_loss: 0.9383 - val_accuracy: 0.8812
Epoch 2/3
964/964 [==============================] - 2564s 3s/step - loss: 0.9249 - accuracy: 0.8816 
- val_loss: 0.9284 - val_accuracy: 0.8827
Epoch 3/3
964/964 [==============================] - 2585s 3s/step - loss: 0.9164 - accuracy: 0.8831 
- val_loss: 0.9245 - val_accuracy: 0.8830
Last measured accuracy score:  0.8830223083496094
[<matplotlib.lines.Line2D object at 0x000002E9BE0C9480>]
[<matplotlib.lines.Line2D object at 0x000002E9BE0C9780>]
Text(0.5, 1.0, 'Accuracy of English to Dutch translator (word-based, Word2Vec embedding, with attention)')
Text(0, 0.5, 'accuracy')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BE0A9BD0>
[<matplotlib.lines.Line2D object at 0x000002E9BE111300>]
[<matplotlib.lines.Line2D object at 0x000002E9BE110E50>]
Text(0.5, 1.0, 'model loss')
Text(0, 0.5, 'loss')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BE0CB1F0>

MODEL: English to Dutch translator (word-based, Glove embedding, with attention)
Model: "sequential_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 embedding_7 (Embedding)     (None, 211, 300)          5432400

 gru_22 (GRU)                (None, 64)                70272

 repeat_vector_11 (RepeatVec  (None, 212, 64)          0
 tor)

 seq_self_attention (SeqSelf  (None, 212, 64)          4161
 Attention)

 gru_23 (GRU)                (None, 212, 64)           24960

 time_distributed_11 (TimeDi  (None, 212, 25653)       1667445
 stributed)

=================================================================
Total params: 7,199,238
Trainable params: 7,199,238
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
964/964 [==============================] - 2568s 3s/step - loss: 1.3685 - accuracy: 0.8815 
- val_loss: 0.9320 - val_accuracy: 0.8812
Epoch 2/3
964/964 [==============================] - 2569s 3s/step - loss: 0.9212 - accuracy: 0.8823 
- val_loss: 0.9281 - val_accuracy: 0.8829
Epoch 3/3
964/964 [==============================] - 2565s 3s/step - loss: 0.9150 - accuracy: 0.8832 
- val_loss: 0.9236 - val_accuracy: 0.8831
Last measured accuracy score:  0.8830651044845581
[<matplotlib.lines.Line2D object at 0x000002E9BBE806D0>]
[<matplotlib.lines.Line2D object at 0x000002E9BBE675E0>]
Text(0.5, 1.0, 'Accuracy of English to Dutch translator (word-based, Glove embedding, with 
attention)')
Text(0, 0.5, 'accuracy')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BBE67610>
[<matplotlib.lines.Line2D object at 0x000002E9BA934670>]
[<matplotlib.lines.Line2D object at 0x000002E9BA934970>]
Text(0.5, 1.0, 'model loss')
Text(0, 0.5, 'loss')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BBE9C880>

MODEL: English to Dutch translator (word-based)
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 gru (GRU)                   (None, 64)                12864

 repeat_vector (RepeatVector  (None, 212, 64)          0
 )

 gru_1 (GRU)                 (None, 212, 64)           24960

 time_distributed (TimeDistr  (None, 212, 25653)       1667445
 ibuted)

=================================================================
Total params: 1,705,269
Trainable params: 1,705,269
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
964/964 [==============================] - 2341s 2s/step - loss: 1.3783 - accuracy: 0.8815 
- val_loss: 0.9389 - val_accuracy: 0.8812
Epoch 2/3
964/964 [==============================] - 2355s 2s/step - loss: 0.9245 - accuracy: 0.8816 
- val_loss: 0.9331 - val_accuracy: 0.8815
Epoch 3/3
964/964 [==============================] - 2359s 2s/step - loss: 0.9179 - accuracy: 0.8829 
- val_loss: 0.9355 - val_accuracy: 0.8826
Last measured accuracy score:  0.8826014995574951
[<matplotlib.lines.Line2D object at 0x000002E9BD3AA680>]
[<matplotlib.lines.Line2D object at 0x000002E9BD3AA980>]
Text(0.5, 1.0, 'Accuracy of English to Dutch translator (word-based)')
Text(0, 0.5, 'accuracy')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BD3A95D0>
[<matplotlib.lines.Line2D object at 0x000002E9BD3F83D0>]
[<matplotlib.lines.Line2D object at 0x000002E9BD3F8CA0>]
Text(0.5, 1.0, 'model loss')
Text(0, 0.5, 'loss')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BD3A9330>

MODEL: English to Dutch translator (word-based, Word2Vec embedding)
Model: "sequential_9"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 embedding_5 (Embedding)     (None, 211, 300)          5432400

 gru_18 (GRU)                (None, 64)                70272

 repeat_vector_9 (RepeatVect  (None, 212, 64)          0
 or)

 gru_19 (GRU)                (None, 212, 64)           24960

 time_distributed_9 (TimeDis  (None, 212, 25653)       1667445
 tributed)

=================================================================
Total params: 7,195,077
Trainable params: 7,195,077
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
964/964 [==============================] - 2438s 3s/step - loss: 1.3781 - accuracy: 0.8815 
- val_loss: 0.9345 - val_accuracy: 0.8812
Epoch 2/3
964/964 [==============================] - 2435s 3s/step - loss: 0.9226 - accuracy: 0.8821 
- val_loss: 0.9259 - val_accuracy: 0.8827
Epoch 3/3
964/964 [==============================] - 2442s 3s/step - loss: 0.9158 - accuracy: 0.8833 
- val_loss: 0.9242 - val_accuracy: 0.8831
Last measured accuracy score:  0.883099377155304
[<matplotlib.lines.Line2D object at 0x000002E9BA906B30>]
[<matplotlib.lines.Line2D object at 0x000002E9BA7EEE60>]
Text(0.5, 1.0, 'Accuracy of English to Dutch translator (word-based, Word2Vec embedding)') 
Text(0, 0.5, 'accuracy')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BB4E47F0>
[<matplotlib.lines.Line2D object at 0x000002E9BD38AB60>]
[<matplotlib.lines.Line2D object at 0x000002E9BD389CF0>]
Text(0.5, 1.0, 'model loss')
Text(0, 0.5, 'loss')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9BE797580>

MODEL: English to Dutch translator (char-based)
Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 gru_4 (GRU)                 (None, 64)                12864

 repeat_vector_2 (RepeatVect  (None, 1483, 64)         0
 or)

 gru_5 (GRU)                 (None, 1483, 64)          24960

 time_distributed_2 (TimeDis  (None, 1483, 90)         5850
 tributed)

=================================================================
Total params: 43,674
Trainable params: 43,674
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
964/964 [==============================] - 939s 971ms/step - loss: 0.5129 - accuracy: 0.8962 - val_loss: 0.4230 - val_accuracy: 0.8966
Epoch 2/3
964/964 [==============================] - 950s 986ms/step - loss: 0.4190 - accuracy: 0.8972 - val_loss: 0.4216 - val_accuracy: 0.8969
Epoch 3/3
964/964 [==============================] - 950s 986ms/step - loss: 0.4188 - accuracy: 0.8974 - val_loss: 0.4216 - val_accuracy: 0.8970
Last measured accuracy score:  0.897040605545044
[<matplotlib.lines.Line2D object at 0x000002E9B77E1810>]
[<matplotlib.lines.Line2D object at 0x000002E9B77E1B10>]
Text(0.5, 1.0, 'Accuracy of English to Dutch translator (char-based)')
Text(0, 0.5, 'accuracy')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9B77B32B0>
[<matplotlib.lines.Line2D object at 0x000002E9B7830730>]
[<matplotlib.lines.Line2D object at 0x000002E9B7831C30>]
Text(0.5, 1.0, 'model loss')
Text(0, 0.5, 'loss')
Text(0.5, 0, 'epoch')
<matplotlib.legend.Legend object at 0x000002E9B780DB40>

MODEL: Dutch to English translator (word-based)
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 gru_2 (GRU)                 (None, 64)                12864

 repeat_vector_1 (RepeatVect  (None, 211, 64)          0
 or)

 gru_3 (GRU)                 (None, 211, 64)           24960

 time_distributed_1 (TimeDis  (None, 211, 18108)       1177020
 tributed)

=================================================================
Total params: 1,214,844
Trainable params: 1,214,844
Non-trainable params: 0
_________________________________________________________________
Epoch 1/3
 12/964 [..............................] - ETA: 26:17 - loss: 9.7697 - accuracy: 0.8820Traceback (most recent call last):
  File "<stdin>", line 7, in <module>
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\keras\utils\traceback_utils.py", line 64, in error_handler
    return fn(*args, **kwargs)
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\keras\engine\training.py", line 1409, in fit
    tmp_logs = self.train_function(iterator)
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\tensorflow\python\util\traceback_utils.py", line 150, in error_handler
    return fn(*args, **kwargs)
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\tensorflow\python\eager\def_function.py", line 915, in __call__
    result = self._call(*args, **kwds)
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\tensorflow\python\eager\def_function.py", line 947, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\tensorflow\python\eager\function.py", line 2453, in __call__
    return graph_function._call_flat(
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\tensorflow\python\eager\function.py", line 1860low\python\eager\function.py", line 1860, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\tensorflow\python\eager\function.py", line 497,low\python\eager\function.py", line 497, in call
    outputs = execute.execute(
  File "C:\Users\sidla\Documents\GitHub\machine-translation\.venv\lib\site-packages\tensorflow\python\eager\execute.py", line 54, ilow\python\eager\execute.py", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
>>> def toText(sentence, vocab):
...     return ""
...
>>>                                                                                        